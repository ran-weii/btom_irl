algo: rambo
exp_path: ../../exp/mujoco/rl
data_path: ../../data/d4rl
cp_path: none

# data args
num_samples: 2000000
norm_obs: True
norm_rwd: False

# dynamics args
ensemble_dim: 7
topk: 5
m_hidden_dim: 200
m_num_hidden: 3
m_activation: silu
residual: True
min_std: 0.04
max_std: 1.6
obs_penalty: 1.
adv_penalty: 0.1
adv_rollout_steps: 10
adv_action_deterministic: True
adv_include_entropy: False
adv_clip_max: 40.
norm_advantage: True
decay: [0.000025, 0.00005, 0.000075, 0.000075, 0.0001]

# policy args
a_hidden_dim: 256
a_num_hidden: 2
a_activation: relu
gamma: 0.99
beta: 1.
polyak: 0.995
tune_beta: True

# training args
buffer_size: 2000000
batch_size: 256
rollout_batch_size: 8000
rollout_deterministic: False
rollout_min_steps: 40
rollout_max_steps: 40
rollout_min_epoch: 50
rollout_max_epoch: 200
model_retain_epochs: 5
real_ratio: 0.5
eval_ratio: 0.2
m_steps: 1000
a_steps: 1
lr_a: 3.e-4
lr_c: 3.e-4
lr_m: 3.e-4
grad_clip: 1000.

# rollout args
pretrain_steps: 0
num_pretrain_samples: 100000
epochs: 1
max_steps: 1000
steps_per_epoch: 1000
sample_model_every: 250
update_model_every: 1000
update_policy_every: 1
cp_every: 10
cp_intermediate: False
num_eval_eps: 10
eval_deterministic: True
verbose: 50
render: False